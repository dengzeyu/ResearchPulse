# Paper Feed

AI-powered research paper aggregator that generates daily insights, research ideas, and trending topics. Runs as a Docker container and publishes to a static blog.

## Core Structure

```
/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fetchers/
â”‚   â”‚   â”œâ”€â”€ arxiv.py       # arXiv API
â”‚   â”‚   â”œâ”€â”€ scholar.py     # Google Scholar
â”‚   â”‚   â”œâ”€â”€ semantic.py    # Semantic Scholar
â”‚   â”‚   â”œâ”€â”€ pubmed.py      # PubMed
â”‚   â”‚   â”œâ”€â”€ author.py      # Track specific authors
â”‚   â”‚   â”œâ”€â”€ citations.py   # Track paper citations
â”‚   â”‚   â””â”€â”€ journals.py    # Scrape journal websites
â”‚   â”œâ”€â”€ social/
â”‚   â”‚   â”œâ”€â”€ twitter.py     # Twitter/X API
â”‚   â”‚   â”œâ”€â”€ linkedin.py    # LinkedIn API
â”‚   â”‚   â”œâ”€â”€ wechat.py      # WeChat public accounts
â”‚   â”‚   â”œâ”€â”€ reddit.py      # Reddit API
â”‚   â”‚   â””â”€â”€ hackernews.py  # HackerNews API
â”‚   â”œâ”€â”€ analyzer.py        # LLM-powered paper analysis
â”‚   â”œâ”€â”€ insights.py        # Generate research ideas & hot topics
â”‚   â”œâ”€â”€ processor.py       # Filter, rank, deduplicate papers
â”‚   â””â”€â”€ generator.py       # Static site generation
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html         # Blog homepage template
â”‚   â”œâ”€â”€ daily.html         # Daily feed page template
â”‚   â””â”€â”€ paper.html         # Individual paper card
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ tracking.yaml      # Keywords, authors, papers, journals to track
â”‚   â”œâ”€â”€ llm.yaml           # LLM provider configs
â”‚   â””â”€â”€ social.yaml        # Social media configs
â”œâ”€â”€ output/                # Generated static site
â”œâ”€â”€ .env                   # API keys and secrets
â”œâ”€â”€ .env.example           # Example configuration
â”œâ”€â”€ Dockerfile
â””â”€â”€ docker-compose.yml
```

## Architecture

### 1. Paper Collection
**Academic Sources:**
- arXiv API
- Google Scholar (via `scholarly` or SerpAPI)
- Semantic Scholar API
- PubMed API

**Social Signals:**
- **Twitter/X API** - trending research hashtags, paper discussions
- **LinkedIn API** - professional network posts, researcher updates
- **WeChat Public Accounts** - Chinese research community articles
- **Reddit API** - r/MachineLearning, r/science posts
- **HackerNews API** - paper discussions

### 2. AI Analysis
**LLM Integration (configurable):**
- **Claude** (Anthropic API)
- **GPT-4** (OpenAI API)
- **Gemini** (Google API)
- **Ollama** (local models)

**Analysis Tasks:**
- Summarize papers (3-5 sentences)
- Extract key contributions
- Identify methodology & results
- Generate research ideas (5 per day)
- Detect hot topics & trends

### 3. Static Site Generation
**Output:** Stylish blog-style static website
- Daily feed pages
- Paper cards with AI summaries
- Research ideas section
- Trending topics dashboard
- Social buzz indicators

## Key Components

**src/analyzer.py**
```python
class LLMAnalyzer:
    def __init__(self, provider: str = "claude")  # claude|openai|gemini|ollama

    def summarize_paper(self, paper: Paper) -> str
    def extract_contributions(self, paper: Paper) -> List[str]
    def generate_research_ideas(self, papers: List[Paper]) -> List[str]
    def identify_hot_topics(self, papers: List[Paper]) -> List[Topic]
```

**src/insights.py**
```python
def generate_daily_insights(papers: List[Paper], social_data: dict) -> Insights:
    """Generate 5 research ideas and hot topics"""

def rank_by_social_buzz(papers: List[Paper], social_data: dict) -> List[Paper]:
    """Rank papers by social media mentions"""
```

**src/generator.py**
```python
def generate_static_site(feed: DailyFeed, output_dir: str):
    """Generate static HTML/CSS/JS blog"""
```

**src/fetchers/author.py**
```python
class AuthorTracker:
    def fetch_author_papers(self, author: str, days: int) -> List[Paper]:
        """Fetch recent papers by specific author"""
```

**src/fetchers/citations.py**
```python
class CitationTracker:
    def fetch_citing_papers(self, arxiv_id: str, days: int) -> List[Paper]:
        """Fetch papers citing a key paper"""
```

**src/fetchers/journals.py**
```python
class JournalScraper:
    def fetch_journal_papers(self, journal_url: str, sections: List[str]) -> List[Paper]:
        """Scrape latest papers from journal website"""
```

**src/social/twitter.py**
```python
class TwitterFetcher:
    def get_trending_papers(self, hashtags: List[str], users: List[str]) -> List[Tweet]
```

**src/social/linkedin.py**
```python
class LinkedInFetcher:
    def get_posts(self, keywords: List[str], companies: List[str]) -> List[Post]
```

**src/social/wechat.py**
```python
class WeChatFetcher:
    def get_articles(self, account_ids: List[str]) -> List[Article]:
        """Fetch articles from WeChat public accounts"""
```

## Configuration

**config/llm.yaml**
```yaml
provider: claude  # claude|openai|gemini|ollama
models:
  claude: claude-3-5-sonnet-20241022
  openai: gpt-4-turbo
  gemini: gemini-pro
  ollama: llama3.1:8b
tasks:
  summarization: true
  research_ideas: true
  hot_topics: true
```

**config/tracking.yaml**
```yaml
# Track by keywords
keywords:
  - area: "Machine Learning"
    terms: ["neural networks", "transformers", "LLMs", "diffusion models"]
    sources: ["arxiv", "google_scholar", "semantic_scholar"]
  - area: "Quantum Computing"
    terms: ["quantum algorithms", "qubits", "quantum error correction"]
    sources: ["arxiv", "nature", "science"]

# Track specific authors
authors:
  - name: "Yoshua Bengio"
    affiliations: ["Mila", "University of Montreal"]
  - name: "Geoffrey Hinton"
    affiliations: ["Google", "University of Toronto"]
  - name: "Yann LeCun"
    affiliations: ["Meta", "NYU"]

# Track papers citing these influential works
key_papers:
  - title: "Attention Is All You Need"
    arxiv_id: "1706.03762"
    track_citations: true
  - title: "BERT: Pre-training of Deep Bidirectional Transformers"
    arxiv_id: "1810.04805"
    track_citations: true

# Track specific journals
journals:
  - name: "Nature"
    url: "https://www.nature.com"
    sections: ["articles", "letters"]
  - name: "Science"
    url: "https://www.science.org"
  - name: "Nature Machine Intelligence"
    url: "https://www.nature.com/natmachintell"
  - name: "JMLR"
    url: "https://jmlr.org"
```

**config/social.yaml**
```yaml
twitter:
  hashtags: ["#MachineLearning", "#AI", "#DeepLearning", "#NeurIPS", "#ICLR"]
  track_users: ["ylecun", "goodfellow_ian", "karpathy"]

linkedin:
  track_keywords: ["AI research", "machine learning paper", "new publication"]
  track_companies: ["DeepMind", "OpenAI", "Anthropic", "Meta AI"]

wechat:
  public_accounts:
    - name: "æœºå™¨ä¹‹å¿ƒ"  # Machine Heart
      account_id: "almosthuman2014"
    - name: "AIç§‘æŠ€è¯„è®º"  # AI Tech Review
      account_id: "aitechtalk"
    - name: "æ–°æ™ºå…ƒ"  # AI Era
      account_id: "AI_era"

reddit:
  subreddits: ["MachineLearning", "deeplearning", "ArtificialIntelligence"]
  min_upvotes: 50

hackernews:
  keywords: ["paper", "research", "arxiv"]
  min_points: 100
```

## Docker Deployment

**Dockerfile**
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "main.py"]
```

**docker-compose.yml**
```yaml
services:
  paper-feed:
    build: .
    env_file:
      - .env
    volumes:
      - ./output:/app/output
      - ./config:/app/config
    restart: always
```

## Configuration Files

**.env** (API keys and secrets)
```bash
# LLM APIs (choose one or more)
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...
OLLAMA_HOST=http://localhost:11434

# Paper Sources
SERPAPI_KEY=...
SEMANTIC_SCHOLAR_KEY=...

# Social Media
TWITTER_API_KEY=...
TWITTER_API_SECRET=...
TWITTER_BEARER_TOKEN=...
LINKEDIN_API_KEY=...
LINKEDIN_API_SECRET=...
WECHAT_APP_ID=...
WECHAT_APP_SECRET=...
REDDIT_CLIENT_ID=...
REDDIT_CLIENT_SECRET=...

# Journal Scraping (if needed)
SPRINGER_API_KEY=...
ELSEVIER_API_KEY=...
```

**.env.example** (Template for users)
```bash
# Copy this to .env and fill in your keys
ANTHROPIC_API_KEY=
OPENAI_API_KEY=
SERPAPI_KEY=
# ... (all keys with empty values)
```

## Deployment

```bash
# 1. Setup environment
cp .env.example .env
# Edit .env with your API keys

# 2. Configure tracking
# Edit config/tracking.yaml with your keywords, authors, papers, journals

# 3. Build container
docker-compose build

# 4. Run container
docker-compose up -d

# 5. View logs
docker logs -f paper-feed

# 6. Serve static output
docker run -p 8080:80 -v ./output:/usr/share/nginx/html nginx
```

## Daily Workflow

1. **Fetch papers** from multiple sources:
   - Keyword searches (arXiv, Google Scholar, Semantic Scholar, PubMed)
   - Author publications (tracked authors)
   - Citation tracking (papers citing key works)
   - Journal websites (Nature, Science, etc.)
2. **Collect social data** from Twitter/X, LinkedIn, WeChat, Reddit, HackerNews
3. **Deduplicate & filter** papers
4. **Analyze with LLM**: summarize, extract insights
5. **Rank papers** by relevance + social buzz + author reputation
6. **Generate insights**: 5 research ideas + hot topics
7. **Build static site** with styled templates
8. **Deploy to server** (nginx/GitHub Pages)

## Output Example

**Daily Feed Page:**
```
ðŸ“… Research Feed - Nov 7, 2024

ðŸ”¥ Hot Topics
- Multimodal LLMs for vision-language tasks
- Quantum error correction breakthroughs
- AI safety alignment research

ðŸ’¡ Research Ideas
1. Combining RL with diffusion models for robotics
2. Cross-lingual reasoning in small language models
...

ðŸ“„ Papers (Ranked by Relevance + Buzz)
[Paper Card: Title, Authors, Summary, Social Mentions, ArXiv Link]
```

## Security

- Store all API keys in `.env` file (never commit to git)
- Add `.env` to `.gitignore`
- Use `.env.example` as template (no real keys)
- Use read-only volumes for config in Docker
- Rate limit API calls to avoid abuse
- Validate and sanitize external data
- Run container as non-root user
